"""
Software Defect Prediction Analysis - Dissertation Final Code
Python Version: 3.8+
Required Packages: pandas, numpy, matplotlib, seaborn, scikit-learn, imbalanced-learn, shap

Author: [Your Name]
Date: [Current Date]
Institution: [Your University]
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
import shap
import warnings
warnings.filterwarnings('ignore')

# ==================== CONFIGURATION ====================
# Set academic style for publication-quality figures
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("Set2")
plt.rcParams['font.size'] = 12
plt.rcParams['font.weight'] = 'bold'
np.random.seed(42)  # For reproducibility

# ==================== DATA GENERATION ====================
def generate_software_metrics_data(n_samples=1500):
    """
    Generate synthetic software metrics data based on established software engineering literature.
    Distributions chosen to reflect real-world software characteristics.
    """
    print("Generating synthetic software metrics data...")
    
    data = {
        # Lines of Code - lognormal distribution common in software metrics
        'loc': np.random.lognormal(4.2, 1.3, n_samples),
        # Cyclomatic complexity - lognormal distribution
        'v_g': np.random.lognormal(2.1, 0.9, n_samples),
        # Essential complexity - lognormal distribution  
        'ev_g': np.random.lognormal(1.6, 0.7, n_samples),
        # Design complexity - lognormal distribution
        'iv_g': np.random.lognormal(1.9, 0.8, n_samples),
        # Branch count - Poisson distribution for discrete counts
        'branch_count': np.random.poisson(12, n_samples),
        # Halstead volume - lognormal for software complexity metric
        'halstead_volume': np.random.lognormal(150, 50, n_samples),
        # Number of operators - Poisson distribution
        'num_operators': np.random.poisson(45, n_samples),
        # Number of operands - Poisson distribution
        'num_operands': np.random.poisson(85, n_samples),
    }
    
    X = pd.DataFrame(data)
    
    # Generate defect probability based on weighted feature contributions
    # Reflects established relationships in software defect prediction literature
    defect_probability = (
        0.25 * (X['v_g'] / X['v_g'].max()) +  # Cyclomatic complexity: strong predictor
        0.20 * (X['loc'] / X['loc'].max()) +  # Size metric: moderate predictor
        0.15 * (X['iv_g'] / X['iv_g'].max()) +  # Design complexity: moderate predictor
        0.15 * (X['halstead_volume'] / X['halstead_volume'].max()) +  # Complexity metric
        0.10 * (X['branch_count'] / X['branch_count'].max()) +  # Control flow complexity
        0.15 * np.random.random(n_samples)  # Random noise/unexplained variance
    )
    
    # Convert probabilities to binary labels (defective/non-defective)
    y = (defect_probability > 0.55).astype(int)
    
    print(f"Dataset generated: {X.shape[0]} samples, {X.shape[1]} features")
    print(f"Defect rate: {y.mean():.3f} ({y.sum()} defective modules)")
    
    return X, y

# ==================== DATA PREPROCESSING ====================
def preprocess_data(X, y, test_size=0.3, random_state=42):
    """
    Preprocess data: split, balance with SMOTE, and scale features.
    """
    print("\nPreprocessing data...")
    
    # Stratified split to maintain class distribution
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=random_state, stratify=y
    )
    
    print(f"Training set: {X_train.shape[0]} samples")
    print(f"Test set: {X_test.shape[0]} samples")
    print(f"Original training class distribution: {pd.Series(y_train).value_counts().to_dict()}")
    
    # Handle class imbalance using SMOTE
    smote = SMOTE(random_state=random_state)
    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
    
    print(f"Balanced training class distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}")
    
    # Standardize features for models sensitive to scale
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_balanced)
    X_test_scaled = scaler.transform(X_test)
    
    return X_train_scaled, X_test_scaled, y_train_balanced, y_test, scaler

# ==================== MODEL TRAINING ====================
def train_models(X_train, y_train):
    """
    Train multiple classification models for comparative analysis.
    """
    print("\nTraining machine learning models...")
    
    models = {
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'Support Vector Machine': SVC(probability=True, random_state=42),
        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
    }
    
    for name, model in models.items():
        model.fit(X_train, y_train)
        print(f"✓ {name} trained successfully")
    
    return models

# ==================== MODEL EVALUATION ====================
def evaluate_models(models, X_test, y_test):
    """
    Comprehensive evaluation of trained models using multiple metrics.
    """
    print("\nEvaluating model performance...")
    
    results = {}
    for name, model in models.items():
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]
        
        results[name] = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred),
            'auc': roc_auc_score(y_test, y_pred_proba),
            'model': model,
            'predictions': y_pred,
            'probabilities': y_pred_proba
        }
        
        print(f"✓ {name}: AUC = {results[name]['auc']:.3f}, F1 = {results[name]['f1']:.3f}")
    
    return results

# ==================== VISUALIZATION FUNCTIONS ====================
def create_shap_analysis(model, X_test, feature_names, filename='SHAP_Analysis.png'):
    """
    Create SHAP analysis plot for model interpretability.
    """
    try:
        print("Generating SHAP analysis...")
        explainer = shap.TreeExplainer(model)
        shap_values = explainer.shap_values(X_test)
        
        plt.figure(figsize=(12, 8))
        shap.summary_plot(shap_values, X_test, feature_names=feature_names, show=False)
        plt.title("SHAP Feature Importance Analysis\n(Random Forest Model)", fontsize=16, fontweight='bold', pad=20)
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print(f"✓ SHAP analysis saved as {filename}")
    except Exception as e:
        print(f"Error creating SHAP analysis: {e}")

def create_confusion_matrix(y_true, y_pred, filename='Confusion_Matrix.png'):
    """
    Create confusion matrix visualization.
    """
    try:
        print("Generating confusion matrix...")
        cm = confusion_matrix(y_true, y_pred)
        
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True,
                    xticklabels=['Non-Defective', 'Defective'],
                    yticklabels=['Non-Defective', 'Defective'],
                    annot_kws={'size': 14, 'weight': 'bold'})
        plt.title('Confusion Matrix - Random Forest Model', fontsize=16, fontweight='bold', pad=20)
        plt.ylabel('Actual Label', fontsize=14, fontweight='bold')
        plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print(f"✓ Confusion matrix saved as {filename}")
    except Exception as e:
        print(f"Error creating confusion matrix: {e}")

def create_roc_curves(results, y_test, filename='ROC_Curves.png'):
    """
    Create ROC curves for model comparison.
    """
    try:
        print("Generating ROC curves...")
        plt.figure(figsize=(10, 8))
        
        for name, metrics in results.items():
            fpr, tpr, _ = roc_curve(y_test, metrics['probabilities'])
            roc_auc = auc(fpr, tpr)
            plt.plot(fpr, tpr, lw=3, label=f'{name} (AUC = {roc_auc:.3f})')
        
        plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', alpha=0.6, label='Random Classifier')
        plt.xlim([-0.01, 1.01])
        plt.ylim([-0.01, 1.01])
        plt.xlabel('False Positive Rate', fontsize=14, fontweight='bold')
        plt.ylabel('True Positive Rate', fontsize=14, fontweight='bold')
        plt.title('ROC Curve Comparison - Defect Prediction Models', fontsize=16, fontweight='bold', pad=20)
        plt.legend(loc="lower right", fontsize=12)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print(f"✓ ROC curves saved as {filename}")
    except Exception as e:
        print(f"Error creating ROC curves: {e}")

def create_correlation_heatmap(X, y, filename='Feature_Correlation.png'):
    """
    Create feature correlation heatmap.
    """
    try:
        print("Generating correlation heatmap...")
        X_with_target = X.copy()
        X_with_target['defects'] = y
        
        plt.figure(figsize=(12, 10))
        correlation_matrix = X_with_target.corr()
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,
                    square=True, fmt='.2f', cbar_kws={"shrink": .8})
        plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)
        plt.tight_layout()
        plt.savefig(filename, dpi=300, bbox_inches='tight', facecolor='white')
        plt.close()
        print(f"✓ Correlation heatmap saved as {filename}")
    except Exception as e:
        print(f"Error creating correlation heatmap: {e}")

# ==================== RESULTS EXPORT ====================
def export_results(results, X, y, metrics_filename='model_performance.csv', 
                   sample_filename='sample_dataset.csv'):
    """
    Export model results and sample data to CSV files.
    """
    try:
        print("\nExporting results...")
        
        # Export model performance metrics
        metrics_df = pd.DataFrame.from_dict(results, orient='index')
        metrics_df = metrics_df[['accuracy', 'precision', 'recall', 'f1', 'auc']]
        metrics_df.to_csv(metrics_filename)
        print(f"✓ Model performance metrics saved as {metrics_filename}")
        
        # Export sample dataset
        X_with_target = X.copy()
        X_with_target['defects'] = y
        sample_data = X_with_target.sample(100, random_state=42)
        sample_data.to_csv(sample_filename, index=False)
        print(f"✓ Sample dataset saved as {sample_filename}")
        
    except Exception as e:
        print(f"Error exporting results: {e}")

# ==================== MAIN EXECUTION ====================
def main():
    """
    Main execution function for the software defect prediction analysis.
    """
    print("=" * 60)
    print("SOFTWARE DEFECT PREDICTION ANALYSIS")
    print("Dissertation Final Code Execution")
    print("=" * 60)
    
    # 1. Generate data
    X, y = generate_software_metrics_data(n_samples=1500)
    
    # 2. Preprocess data
    X_train_scaled, X_test_scaled, y_train_balanced, y_test, scaler = preprocess_data(X, y)
    
    # 3. Train models
    models = train_models(X_train_scaled, y_train_balanced)
    
    # 4. Evaluate models
    results = evaluate_models(models, X_test_scaled, y_test)
    
    # 5. Create visualizations
    create_shap_analysis(models['Random Forest'], X_test_scaled, X.columns)
    create_confusion_matrix(y_test, results['Random Forest']['predictions'])
    create_roc_curves(results, y_test)
    create_correlation_heatmap(X, y)
    
    # 6. Export results
    export_results(results, X, y)
    
    # 7. Final summary
    print("\n" + "=" * 60)
    print("ANALYSIS COMPLETED SUCCESSFULLY")
    print("=" * 60)
    print("Generated Files:")
    print("  - SHAP_Analysis.png (Feature importance)")
    print("  - Confusion_Matrix.png (Model performance)")
    print("  - ROC_Curves.png (Model comparison)")
    print("  - Feature_Correlation.png (Data analysis)")
    print("  - model_performance.csv (Metrics table)")
    print("  - sample_dataset.csv (Sample data)")
    print("\nBest performing model:")
    best_model = max(results.items(), key=lambda x: x[1]['auc'])
    print(f"  {best_model[0]} (AUC: {best_model[1]['auc']:.3f})")
    print("=" * 60)

# Execute main function
if __name__ == "__main__":
    main()
